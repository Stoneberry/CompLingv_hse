{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook \n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>status</th>\n",
       "      <th>formed_in</th>\n",
       "      <th>genre</th>\n",
       "      <th>theme</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>('M') Inc.</td>\n",
       "      <td>United States</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>(sic)</td>\n",
       "      <td>United States</td>\n",
       "      <td>Split-up</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993-1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>.F.O.A.D.</td>\n",
       "      <td>France</td>\n",
       "      <td>Active</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>Life and Death</td>\n",
       "      <td>2009-present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100 Suns</td>\n",
       "      <td>United States</td>\n",
       "      <td>Active</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12 Days of Anarchy</td>\n",
       "      <td>United States</td>\n",
       "      <td>Split-up</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>Anarchy</td>\n",
       "      <td>1998-2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                name        country    status  formed_in        genre  \\\n",
       "0   1          ('M') Inc.  United States   Unknown     2009.0  Death Metal   \n",
       "1   2               (sic)  United States  Split-up     1993.0  Death Metal   \n",
       "2   3           .F.O.A.D.         France    Active     2009.0  Death Metal   \n",
       "3   4            100 Suns  United States    Active     2004.0  Death Metal   \n",
       "4   5  12 Days of Anarchy  United States  Split-up     1998.0  Death Metal   \n",
       "\n",
       "            theme        active  \n",
       "0             NaN        2009-?  \n",
       "1             NaN     1993-1996  \n",
       "2  Life and Death  2009-present  \n",
       "3             NaN  2004-present  \n",
       "4         Anarchy     1998-2002  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bands.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37723,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['name'])\n",
    "\n",
    "X = df.name.str.replace('∆', 'o')\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных есть названия групп на разных языках, поэтому эту задачу можно решать двумя способами: с учетом категории и без. Категорией в данном случае будем счить письменность, на которой написано название группы. Рассмотрим каждую из этих задач отдельно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATEGORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных много диакритик и небуквенных символов, поэтому список букв чистился. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source - https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = '-./0123456789:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzΑΓΔΕΚΝΡΣΩέήαδηθκλμνοπρτψωόώІЇАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЪЭЮабвгдежзийклмнопрстуфхцчшъыьэяёクステノフモルヴ中人修健兀入冥出北卸古台吐呕咒围地复夜大天妖守射尸屠川巨彘影懺战戮手散斧昏晦术杀東梦楽死殺注活浮混火烂烟猝猿甲界症痋祝神禁福空突结羅者而胄脱腐色苔藓虎虐虚蛇血行覆謎诗语豚败轡邪郁針鉄铁閃陈雾霾靈颠餮饕骨魇黑 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = []\n",
    "\n",
    "\n",
    "for index, item in enumerate(X):\n",
    "    if item == 'שְׁאוֹל':\n",
    "        continue\n",
    "    \n",
    "    ans = unicodeToAscii(item)\n",
    "    if ans != '' and ans != ' ':  \n",
    "        X_.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение письменности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphabet_detector import AlphabetDetector\n",
    "\n",
    "ad = AlphabetDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'THAI'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.detect_alphabet(u'าาา')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?','[', ']', '|']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LATIN', 'INTEGER', 'GREEK', 'CYRILLIC', 'CJK', 'KATAKANA']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "    \n",
    "\n",
    "category_lines = defaultdict(list)\n",
    "all_categories = []\n",
    "\n",
    "for line in X_:\n",
    "    \n",
    "    if all(item in s for item in line):\n",
    "        category = 'INTEGER'\n",
    "    else:\n",
    "        category = ad.detect_alphabet(line)\n",
    "        category = list(category)[0]\n",
    "    \n",
    "    if category not in all_categories:\n",
    "        all_categories.append(category)\n",
    "    category_lines[category].append(line)\n",
    "\n",
    "n_letters = len(all_letters) + 1\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть также группы, которые названы чисто цифрами. Выделим их в отдельную категорию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1917',\n",
       " '602',\n",
       " '9',\n",
       " '1:34',\n",
       " '420',\n",
       " '11:34',\n",
       " '12.7',\n",
       " '29/09',\n",
       " '6425',\n",
       " '731',\n",
       " '762',\n",
       " '908',\n",
       " '1914',\n",
       " '4.6',\n",
       " '468']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_lines['INTEGER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['モノノフ', 'ルテクス']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_lines['KATAKANA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_category(letter):\n",
    "    \n",
    "    global s, all_categories\n",
    "    \n",
    "    if letter in s:\n",
    "        category = 'INTEGER'\n",
    "    else:\n",
    "        category = ad.detect_alphabet(letter)\n",
    "        category = list(category)[0]\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = tt.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = tt.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return tt.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingPair():\n",
    "    \n",
    "    global all_letters\n",
    "    \n",
    "    letter = random.choice(all_letters)\n",
    "    category = define_category(letter)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line\n",
    "\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = tt.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def inputTensor(line):\n",
    "    tensor = tt.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return tt.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, category_tensor, input_line_tensor, target_line_tensor):\n",
    "    \n",
    "    rnn.train()\n",
    "    \n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15\n",
    "\n",
    "def sample(rnn, max_length):\n",
    "    \n",
    "    start_letter = random.choice(all_letters)\n",
    "    category = define_category(start_letter)\n",
    "    \n",
    "    with tt.no_grad():  \n",
    "        \n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "           \n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "    \n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return category, output_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef643ebcd01f4045b7cb0826fd031901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.0005\n",
    "optimizer = optim.Adam(rnn.parameters(), learning_rate)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 \n",
    "\n",
    "t_par =  tqdm_notebook(range(1, n_iters + 1))\n",
    "\n",
    "for itr in t_par:\n",
    "    \n",
    "    output, loss = train(rnn, *randomTrainingExample())\n",
    "    total_loss += loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CJK', '猿')\n",
      "('LATIN', 'm')\n",
      "('LATIN', 'f')\n",
      "('GREEK', 'έ')\n",
      "('LATIN', 't')\n",
      "('GREEK', 'Ε')\n",
      "('CJK', '胄')\n",
      "('CJK', '呕')\n",
      "('CJK', '古')\n",
      "('CYRILLIC', 'э')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sample(rnn, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем  генерировать в рамках латиницы, так как больше всего названий. Эта часть обрабатывалась на Kaggle.\n",
    "source - https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58fdf667f312a7f382f13f6bfc4fa6a53c819e62"
   },
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/Stoneberry/Desktop/assigment_7_Kostyanitsyna/text_table4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_uuid": "7b27bf83a2bdcbfe82ef7110a78f13e33df93658"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>M Inc.</td>\n",
       "      <td>M Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sic</td>\n",
       "      <td>sic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>.F.O.A.D.</td>\n",
       "      <td>.F.O.A.D.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100 Suns</td>\n",
       "      <td>100 Suns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12 Days of Anarchy</td>\n",
       "      <td>12 Days of Anarchy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               label                text\n",
       "0           0              M Inc.              M Inc.\n",
       "1           1                 sic                 sic\n",
       "2           2           .F.O.A.D.           .F.O.A.D.\n",
       "3           3            100 Suns            100 Suns\n",
       "4           4  12 Days of Anarchy  12 Days of Anarchy"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05a93d5556bbe249e6cbf39465bc5c0b664ea813"
   },
   "source": [
    "В данных есть нелатинские символы, до этого я уже отсеяла полностью нелатинские названия, но остались вские смешанные названия. Уберем их, чтобы не было названий типа \"thуыа\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_uuid": "a639a4d5ce15514199de657f259fa9a7091f1c5b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = []\n",
    "s = 'йцукенгшщзхъёфывапролджэячсмитьбюΔΝΩαμψ覆颠'\n",
    "\n",
    "for line in df.text:\n",
    "    if all(item not in s for item in line) and len(line) > 2:\n",
    "        line = re.sub('\\n', '', line)\n",
    "        if line[-1] == ' ':\n",
    "            line = line[:-1]\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_uuid": "f838aa27e9a22e057c49d73464ffd974a89358f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -./0123456789:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<>#'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters = set()\n",
    "\n",
    "for index, i in enumerate(data):\n",
    "    if index != 0: all_letters |= set(i)\n",
    "    else: all_letters = set(i)\n",
    "\n",
    "all_letters = sorted(all_letters)\n",
    "all_letters += ['<', '>', '#']\n",
    "''.join(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "_uuid": "0fc3f3669168f85a21062747bc1109f6e1b44186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 51)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters = len(all_letters)\n",
    "max_len = len(max(data, key=lambda x: len(x))) + 1\n",
    "\n",
    "n_letters, max_len "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3eec1f53e2526efd10fa409f63b381ceb8056ce1"
   },
   "source": [
    "## ОБРАБОТКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "_uuid": "fb949a57d092d1c44577400bac90f2e14d08aa7f"
   },
   "outputs": [],
   "source": [
    "def char_tensor(string, typ='start'):\n",
    "    \n",
    "    tensor = tt.zeros(max_len).long()\n",
    "    \n",
    "    if typ == 'start': string = '<'+ string\n",
    "    else: string = string + '>'\n",
    "        \n",
    "    length = len(string)\n",
    "    \n",
    "    for c in range(max_len):\n",
    "        if c < length:\n",
    "            tensor[c] = all_letters.index(string[c])\n",
    "        elif c >= length:\n",
    "            tensor[c] = all_letters.index('#') # padding\n",
    "\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "971d7c0c8bbe9ca126b75ea2ba8b7354a8d99e5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([67, 59, 60, 58, 49, 54, 47, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69,\n",
       "        69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69,\n",
       "        69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tensor('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "_uuid": "22c40a1b061b4c71fe96282b130ccaaabb6c4d86"
   },
   "outputs": [],
   "source": [
    "def inp_out(string):\n",
    "    inp = char_tensor(string[:-1], typ='start')\n",
    "    target = char_tensor(string[1:], typ='end')\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7eb7b52779d0907ac074568d258b5c6e4def3e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([67, 59, 60, 58, 49, 54, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69,\n",
       "         69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69,\n",
       "         69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69]),\n",
       " tensor([60, 58, 49, 54, 47, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69,\n",
       "         69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69,\n",
       "         69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_out('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "07fa4b6e7f63911a08e0a2dcf42f2d4c513245c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_out('string')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "7d65a2cbc928fbda38f03ac24b6ec35ba4dfd709"
   },
   "outputs": [],
   "source": [
    "all_data = [inp_out(i) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "1d0c5205b48a2b40961f451ff5817577643be4e8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tensor_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_data, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e29db7ee2d9c126435b43bbabc1ba824d8225072"
   },
   "source": [
    "### batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "d0b6eb643255a846e872ae874962a5f87c775726"
   },
   "outputs": [],
   "source": [
    "n_batches = 1000\n",
    "batch_size = 32\n",
    "indices = np.random.choice(list(range(len(all_data))), size=len(all_data))\n",
    "input_bat, output_bat = [], []\n",
    "            \n",
    "for j in range(n_batches):\n",
    "    batch_idx = indices[j: j + batch_size]\n",
    "    inp = [all_data[i][0] for i in batch_idx]\n",
    "    out = [all_data[i][1] for i in batch_idx]\n",
    "    input_bat.append(inp)\n",
    "    output_bat.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "809630f7df2a4456e72465790dbd3c6d5d539f37"
   },
   "outputs": [],
   "source": [
    "batches = [(tt.stack(i),  tt.stack(output_bat[index])) for index, i in enumerate(input_bat)]\n",
    "#output_bat = [tt.stack(i) for i in output_bat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "650f1c064de2cc70537fc37064ce13a5f8828dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 51])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "07d475a6f006e853410d45f696aa491244e0264b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(batches, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "756b9dd580a19c7b84985bd113a5131b80d57761"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[67, 20, 52,  ..., 69, 69, 69],\n",
       "         [67, 19, 60,  ..., 69, 69, 69],\n",
       "         [67, 30, 41,  ..., 69, 69, 69],\n",
       "         ...,\n",
       "         [67, 27, 65,  ..., 69, 69, 69],\n",
       "         [67, 18, 45,  ..., 69, 69, 69],\n",
       "         [67, 16, 45,  ..., 69, 69, 69]]),\n",
       " tensor([[52, 49, 60,  ..., 69, 69, 69],\n",
       "         [60, 45, 58,  ..., 69, 69, 69],\n",
       "         [41, 49, 54,  ..., 69, 69, 69],\n",
       "         ...,\n",
       "         [65, 54, 44,  ..., 69, 69, 69],\n",
       "         [45, 60, 45,  ..., 69, 69, 69],\n",
       "         [45, 52, 46,  ..., 69, 69, 69]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e2a07034d33077952bbe99a52a6323c076103de"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "_uuid": "802261459137b50e8ba022fac4692eedccf24ee0"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop = tt.nn.Dropout(0.15)\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.embed(input)\n",
    "        \n",
    "      #  encoded = self.drop(encoded)\n",
    "        \n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        \n",
    "        # Input: (N, *, in_features)\n",
    "        # Output: (N, *, out_features) \n",
    "        \n",
    "        output = self.fc(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(tt.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "_uuid": "2c35475f4d58b07f04db2d170ebd9b114f07a1bd"
   },
   "outputs": [],
   "source": [
    "def _train_epoch(inp, target, model, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    model.zero_grad()\n",
    "    running_loss = 0\n",
    "    perpl = []\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = model(inp[:,c], hidden)\n",
    "        loss = criterion(output.view(batch_size, -1), target[:,c])\n",
    "        perpl.append(perplexity(loss.item()))\n",
    "    \n",
    "    perp = np.mean(perpl)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return perp\n",
    "\n",
    "def _test_epoch(inp, target, model, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    loss = 0\n",
    "    perpl = []\n",
    "    \n",
    "    with tt.no_grad():\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = model(inp[:,c], hidden)\n",
    "            loss = criterion(output.view(batch_size, -1), target[:,c])\n",
    "            perpl.append(perplexity(loss.item()))\n",
    "    \n",
    "    perp = np.mean(perpl)\n",
    "    return perp\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100, scheduler=None, early_stopping=0):\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(n_epochs)):\n",
    "        \n",
    "        for batch in train_iterator:\n",
    "            x = batch[0]\n",
    "            y = batch[1]\n",
    "            train_per = _train_epoch(x, y, model, optimizer, criterion, epoch)\n",
    "            #train_losses.append(train_loss)\n",
    "            \n",
    "        for batch in valid_iterator:\n",
    "            x = batch[0]\n",
    "            y = batch[1]    \n",
    "            valid_per = _test_epoch(x, y, model, criterion)\n",
    "            #valid_losses.append(valid_loss)\n",
    "        \n",
    "        print('epo:  ' + str(epoch))\n",
    "        print('perp:  ' + str(valid_per))\n",
    "        print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_uuid": "7e16d25134831ed975c4b0d92dfb3d4d2b1ef4a3"
   },
   "outputs": [],
   "source": [
    "def perplexity(x):\n",
    "    return 2**x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "92dfe269efe995733a1775951c3c9ba8aeea954f"
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = max_len \n",
    "\n",
    "\n",
    "model = MyModel(n_letters, hidden_size, n_letters)\n",
    "\n",
    "\n",
    "optimizer = tt.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "2014f21a32fdfaf083fc33ac11df02bfbf2c59fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfed0a95b6594abb96c86e7f23867a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epo:  0\n",
      "perp:  4.330233298689488\n",
      "-----------------------------\n",
      "epo:  1\n",
      "perp:  4.439578485674306\n",
      "-----------------------------\n",
      "epo:  2\n",
      "perp:  4.509594391687567\n",
      "-----------------------------\n",
      "epo:  3\n",
      "perp:  4.537983376399163\n",
      "-----------------------------\n",
      "epo:  4\n",
      "perp:  4.561788085917148\n",
      "-----------------------------\n",
      "epo:  5\n",
      "perp:  4.5728363641702\n",
      "-----------------------------\n",
      "epo:  6\n",
      "perp:  4.573747194109011\n",
      "-----------------------------\n",
      "epo:  7\n",
      "perp:  4.574938014065511\n",
      "-----------------------------\n",
      "epo:  8\n",
      "perp:  4.576087942567768\n",
      "-----------------------------\n",
      "epo:  9\n",
      "perp:  4.576880003242535\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_train(model, train, valid, criterion, optimizer, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2befec103dc69b4099a80fca819891691dbd4dc2"
   },
   "source": [
    "## SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "_uuid": "5f4f225ded3a0ae4279e1dd85b1f8e00ba532047"
   },
   "outputs": [],
   "source": [
    "def categoryTensor(letter):\n",
    "    \n",
    "    if len(letter) != 1:\n",
    "        raise ValueError\n",
    "        \n",
    "    tensor = tt.zeros(1).long()\n",
    "    tensor[0] = all_letters.index(letter)\n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "_uuid": "c0dc8a7cee64fac3666248b5b5c9652fb24648b7"
   },
   "outputs": [],
   "source": [
    "def sampling(model, prime_str='<', predict_len=10, temperature=0.8):\n",
    "    \n",
    "    def samp(model, prime_str='<', predict_len=10, temperature=0.8):\n",
    "        \n",
    "        hidden = model.init_hidden(1)\n",
    "        inp = categoryTensor(prime_str)\n",
    "        predicted = ''\n",
    "        miss_count = 0\n",
    "        \n",
    "        for p in range(predict_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_i = tt.multinomial(output_dist, 1)[0]\n",
    "\n",
    "            predicted_char = all_letters[top_i]\n",
    "        \n",
    "            if predicted != '' and predicted_char == '>': break\n",
    "            if predicted != '' and predicted_char == '#': break\n",
    "            if predicted == '' and predicted_char == '#': \n",
    "                inp = inp\n",
    "                miss_count += 1\n",
    "            if predicted == '' and predicted_char == '>': \n",
    "                inp = inp\n",
    "                miss_count += 1\n",
    "            else:\n",
    "                predicted += predicted_char\n",
    "                inp = categoryTensor(predicted_char)\n",
    "        \n",
    "        return predicted, miss_count\n",
    "    \n",
    "    predicted, miss_count = samp(model, prime_str=prime_str, predict_len=predict_len, temperature=temperature)\n",
    "    \n",
    "    if miss_count != 0:\n",
    "        pred, miss_count = samp(model, prime_str=prime_str, predict_len=miss_count, temperature=temperature)\n",
    "        predicted += pred\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "86d2a8f080d200808fa330cebfb6fbc2cba0120d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L Qymt2pq2\n",
      "K21ENq1TA:\n",
      "iyFYX9Qfxk\n",
      "ctjTc<G8k3\n",
      "e:pYHfA28 \n",
      "h8CkQhI/pO\n",
      "/FJdFoeE:c\n",
      "Jwu94c5jlg\n",
      "Qyht89:5fH\n",
      "90BsJOkNgx\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampling(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70af1d94d96ab18101b7dad910013ffdd0f198bf"
   },
   "source": [
    "Результаты плохие, попробуем выбирать рандомные примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31fe8ad7928248dec65d08cec0b56f7606f26295"
   },
   "source": [
    "## RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "746768d4a2d998467b6dc36cd85ec1dc017a5909"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_uuid": "ad1c5525697e54b1cb7ea3a4573870038c0321e3"
   },
   "outputs": [],
   "source": [
    "def nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100, scheduler=None, early_stopping=0):\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(n_epochs)):\n",
    "        \n",
    "        x, y = random.choice(train_iterator)\n",
    "        train_per = _train_epoch(x, y, model, optimizer, criterion, epoch)\n",
    "        \n",
    "            \n",
    "        x, y = random.choice(valid_iterator)    \n",
    "        valid_per = _test_epoch(x, y, model, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "6740847703b566beac142cc0e495f9785f726fed"
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = max_len \n",
    "\n",
    "\n",
    "model = MyModel(n_letters, hidden_size, n_letters)\n",
    "\n",
    "\n",
    "optimizer = tt.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "33e17ed6019cbca0986d8ef7a18a91707a62a04e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4761d961d343698045e48c0be8147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_train(model, train, valid, criterion, optimizer, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "153369f415b0e1f8a9bb3b120757e23b16cd27ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vWoeCoIyk8\n",
      "9EtsKZUOvJ\n",
      " dPytVI FA\n",
      "f7naDot fH\n",
      "Wca3TZzih<\n",
      "cnDbf6Jp\n",
      "2JPtZ5sTby\n",
      "YcJ\n",
      "-UGt-g6MKA\n",
      "2YsL3hgfA3\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampling(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69c81a41c380ec94c9747de53a5c1f33ebbffdf0"
   },
   "source": [
    " Все еще плохо, попробуем убрать увеличить размер текста "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d472091b6fb6d45c3d34622a80487fc674799308"
   },
   "source": [
    "Уберем тексты с  числами и пункутацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "_uuid": "2d9cb40ab34bd2a9cb8ea3842049838baf98f94c"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = list(string.ascii_letters + '<' + '>' + '#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "_uuid": "86bd4d86fc6a17759be71b0ae6626fa4f7ee4cab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ<>#'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "_uuid": "f8785c8b450520b39a0c33b1fce0296a76f2530f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters = len(all_letters)\n",
    "n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "_uuid": "cb20d46ed80470676ec72505bda69b1cbbf3681d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data2 = []\n",
    "\n",
    "for line in data:\n",
    "    if all(item in all_letters for item in line) and len(line) > 2:\n",
    "        data2.append(line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "_uuid": "d95c6c90e4885a6a09bd45a6a63536f32b55b569"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sic', 'abaddon']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8116cdd342605cdbe54ce7b7fd306a5aaecc5af3"
   },
   "source": [
    "## MAX_LEN CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "_uuid": "ec4c3ebd5a501c12e9c6c78f666c21cb1bb551da"
   },
   "outputs": [],
   "source": [
    "max_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "_uuid": "e70a27a0c78cdf1b5dd4330129b0320fbdcff5b5"
   },
   "outputs": [],
   "source": [
    "train = ''\n",
    "valid = ''\n",
    "\n",
    "indices = np.random.choice(list(range(len(data2))), size=len(data2))\n",
    "stop = int(len(data2) * 0.8)\n",
    "count = 0\n",
    "\n",
    "for i in indices:\n",
    "    if count >= stop:\n",
    "        valid += '<' + data2[i] + '>'\n",
    "    else:\n",
    "        train += '<' + data2[i] + '>'\n",
    "        count += 1\n",
    "\n",
    "new_data = train + valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "_uuid": "470734c8585a6e738e00d3635ffe744d38fba95f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210631"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "_uuid": "f990b1fc215873c396303a662ee231595bdaf863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168404"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "_uuid": "cbe9dc43b8cbbd6a8c60be1040f0c4a1678e48cd"
   },
   "outputs": [],
   "source": [
    "def new_char_tensor(string):\n",
    "    tensor = tt.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_letters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "_uuid": "06d8c8e32974f244b389b3b356a8ea4ef06c2bf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 19, 17,  8, 13,  6])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_char_tensor('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "_uuid": "a5d1720be79088780c89d2352db1b8f5496ecca8"
   },
   "outputs": [],
   "source": [
    "def data_set(max_len, batch_size, new_data):\n",
    "    \n",
    "    inp = tt.LongTensor(batch_size, max_len)\n",
    "    out = tt.LongTensor(batch_size, max_len)\n",
    "    indices = np.random.choice(list(range(len(new_data))), size=len(new_data)//max_len)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for j in indices:\n",
    "        if i ==  batch_size: break\n",
    "        \n",
    "        if j + max_len + 1 > len(new_data):\n",
    "            continue\n",
    "        else:\n",
    "            text = new_data[j: j + max_len + 1]\n",
    "            input = new_char_tensor(text[:-1])\n",
    "            output = new_char_tensor(text[1:])\n",
    "            inp[i] = input\n",
    "            out[i] = output\n",
    "            i += 1\n",
    "        \n",
    "    inp = Variable(inp)\n",
    "    out = Variable(out)\n",
    "    return inp, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "_uuid": "8968f016556860534c897e3912633a2323decc57"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "x, y = data_set(max_len, batch_size, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "_uuid": "9af4f7b5a065cc30afe0888cb561c7a0dd787288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<vitriol><'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "_uuid": "92c625b31ec5bff231d06047934b76c620192c1a"
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = max_len \n",
    "\n",
    "\n",
    "model = MyModel(n_letters, hidden_size, n_letters)\n",
    "\n",
    "\n",
    "optimizer = tt.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "_uuid": "8f7940855e9b5910c29c0f21186594830076e6cc"
   },
   "outputs": [],
   "source": [
    "def nn_train3(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100, scheduler=None, early_stopping=0):\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(n_epochs)):\n",
    "        \n",
    "        x, y = data_set(max_len, batch_size, train_iterator)\n",
    "        train_per = _train_epoch(x, y, model, optimizer, criterion, epoch)\n",
    "        \n",
    "            \n",
    "        x, y = data_set(max_len, batch_size, valid_iterator)  \n",
    "        valid_per = _test_epoch(x, y, model, criterion)\n",
    "        print(valid_per)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "_uuid": "02b706455de576addffb5f462336fa26d6592591"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a92171a76294bf592be482ad09441fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.442764369538397\n",
      "11.016835293179643\n",
      "9.049129436091553\n",
      "7.848327039466904\n",
      "7.50372828060921\n",
      "7.32471554633364\n",
      "7.131663477711145\n",
      "6.927224646063294\n",
      "6.750126332605114\n",
      "6.622507866841935\n",
      "6.546723649236074\n",
      "6.4902594435236525\n",
      "6.374162110316436\n",
      "6.248916602804617\n",
      "6.314379205446507\n",
      "6.221398127919374\n",
      "6.1974883750119325\n",
      "6.068850438084599\n",
      "6.074577576790768\n",
      "5.969511381873059\n",
      "5.936788018204073\n",
      "5.822415484276597\n",
      "5.920210211540478\n",
      "6.019472412098903\n",
      "6.021408905936869\n",
      "5.884224793706914\n",
      "5.887912776910287\n",
      "5.902992221751512\n",
      "5.991311687672449\n",
      "5.991876649286415\n",
      "5.862071838637773\n",
      "5.838115705580036\n",
      "5.77286453052311\n",
      "5.820449746194766\n",
      "5.946882546159231\n",
      "5.975299456730269\n",
      "5.995682876310205\n",
      "5.722760027073354\n",
      "5.6769987353101286\n",
      "5.6372679809441895\n",
      "5.487390971070689\n",
      "5.428610345035017\n",
      "5.412538012114933\n",
      "5.476085254679203\n",
      "5.54921799600325\n",
      "5.6251230564579755\n",
      "5.659331489354039\n",
      "5.496087859698343\n",
      "5.507039151621104\n",
      "5.445600088869327\n",
      "5.492245802583982\n",
      "5.355178757466656\n",
      "5.483835135740489\n",
      "5.509476562884253\n",
      "5.4328760996076655\n",
      "5.316562369160437\n",
      "5.273557989498686\n",
      "5.439779050458571\n",
      "5.3574191079167335\n",
      "5.447639422685952\n",
      "5.442615519548075\n",
      "5.410774593677146\n",
      "5.404006142276351\n",
      "5.40064156812125\n",
      "5.48509366547625\n",
      "5.416830696883477\n",
      "5.380833463028499\n",
      "5.294218840569044\n",
      "5.299615056253662\n",
      "5.211447364223233\n",
      "5.220837686138452\n",
      "5.272454258410349\n",
      "5.297505977397024\n",
      "5.268115150640001\n",
      "5.169648700152923\n",
      "5.223457464728355\n",
      "5.224964040120695\n",
      "5.173206885118419\n",
      "5.224384539103492\n",
      "5.167716890079026\n",
      "5.277675634355604\n",
      "5.315904306720969\n",
      "5.344039092883589\n",
      "5.3292857600251144\n",
      "5.323441599767349\n",
      "5.2797483254140225\n",
      "5.208269342682748\n",
      "5.259427195018335\n",
      "5.1834815105210525\n",
      "5.283758969873608\n",
      "5.192399098597791\n",
      "5.276867833711577\n",
      "5.438531587023546\n",
      "5.363264476497444\n",
      "5.396233992799393\n",
      "5.381107978674393\n",
      "5.451221847024713\n",
      "5.453972090180937\n",
      "5.401584221467459\n",
      "5.333504558052289\n"
     ]
    }
   ],
   "source": [
    "nn_train3(model, train, valid, criterion, optimizer, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "_uuid": "68bb0468362a3d76b972edea848dc46f087f2374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensheroh\n",
      "sonel\n",
      "pasliin\n",
      "kerombamon\n",
      "worpy\n",
      "dsioc\n",
      "utsk\n",
      "popumon\n",
      "unte\n",
      "ata\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampling(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b679efe19d7cc90d1bdb3b329ab9d716d8e41c66"
   },
   "source": [
    "уже лучше, увеличим кол-во эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "_uuid": "fed521f863f23047268cb7229fa1b57ed0ea6c15"
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = max_len \n",
    "\n",
    "\n",
    "model = MyModel(n_letters, hidden_size, n_letters)\n",
    "\n",
    "\n",
    "optimizer = tt.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "_uuid": "cfba8fc086ec70e17497aa9e48e7a6c08fee1dcd"
   },
   "outputs": [],
   "source": [
    "def nn_train3(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100, scheduler=None, early_stopping=0):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(n_epochs)):\n",
    "        \n",
    "        x, y = data_set(max_len, batch_size, train_iterator)\n",
    "        train_per = _train_epoch(x, y, model, optimizer, criterion, epoch)\n",
    "        \n",
    "            \n",
    "        x, y = data_set(max_len, batch_size, valid_iterator)  \n",
    "        valid_per = _test_epoch(x, y, model, criterion)\n",
    "        \n",
    "        if epoch == count:\n",
    "            print(valid_per)\n",
    "            count += 100\n",
    "    print(valid_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "_uuid": "6bb156b98aab2d563daa81f1691d23dd7c309836"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b641afe457140338a5f452925dee0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.262463421261863\n",
      "5.197899052188886\n",
      "5.204879074500201\n",
      "5.279704560455544\n",
      "5.062176841685019\n",
      "5.139055042789982\n",
      "5.069818501508898\n",
      "5.110572355752592\n",
      "5.230344013985942\n",
      "5.189163615230626\n",
      "5.1027753414919195\n"
     ]
    }
   ],
   "source": [
    "nn_train3(model, train, valid, criterion, optimizer, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "_uuid": "7e4222349d51c2ef92b6abf4ce79ec18d39fec5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emal\n",
      "onelestrop\n",
      "shymazzor\n",
      "racedathed\n",
      "acrazodete\n",
      "chraphafod\n",
      "wissiste\n",
      "maden\n",
      "usgell\n",
      "dhjate\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampling(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "_uuid": "48fdf30c81f11e0fbf1cdff07eb15a47e6c22c4d"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('model_1000.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21e7db981f72c7bdde7e512ad4f01ec4538d7647"
   },
   "source": [
    "Попробуем с дропаутом теперь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "_uuid": "be6cae0147aff1035d51620132f9de6b2ad9287f"
   },
   "outputs": [],
   "source": [
    "class MyModel2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        \n",
    "        super(MyModel2, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop = tt.nn.Dropout(0.15)\n",
    "\n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.embed(input)\n",
    "        \n",
    "        encoded = self.drop(encoded)\n",
    "        \n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        \n",
    "        #output = self.drop(output) \n",
    "        \n",
    "        output = self.fc(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(tt.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "_uuid": "12347978af8432c0f0de68f06ab2c79b84042560"
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = max_len \n",
    "\n",
    "\n",
    "model2 = MyModel2(n_letters, hidden_size, n_letters)\n",
    "\n",
    "\n",
    "optimizer = tt.optim.Adam(model2.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "_uuid": "da306dc5ad3d295b59e70563adf601dcdf5bdb80"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26bbb62a25849889f46c077deb672ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.269349840922414\n",
      "5.265471763584202\n",
      "5.138988457471356\n",
      "5.165397204902967\n",
      "5.129537323944187\n",
      "5.278714143441692\n",
      "5.08598359373134\n",
      "5.2558068389448325\n",
      "5.187435393140812\n",
      "5.190252445472418\n",
      "5.028386673491473\n"
     ]
    }
   ],
   "source": [
    "nn_train3(model2, train, valid, criterion, optimizer, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "_uuid": "413c088cf774a0d4259837160b862884a6356c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dated\n",
      "neace\n",
      "dyaratoron\n",
      "ortiatii\n",
      "deltici\n",
      "idtian\n",
      "llic\n",
      "newron\n",
      "dare\n",
      "death\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampling(model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "482bd4b02784e3d1d4a4ade3c4ac5049c9132cb0"
   },
   "source": [
    "Кажется, еще лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "_uuid": "f56bec7416abd761dcd011fc1c1b393e85efb10f"
   },
   "outputs": [],
   "source": [
    "with open('model_drop_1000.pickle', 'wb') as handle:\n",
    "    pickle.dump(model2, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36bc0119f5915bf059f4ffa09d65630dd1e5a400"
   },
   "source": [
    "тут я пробовала дров делать и на итог работы rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "_uuid": "f5814a407b0cc5c2b544f4000062904badf69de9"
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = max_len \n",
    "\n",
    "\n",
    "model3 = MyModel2(n_letters, hidden_size, n_letters)\n",
    "\n",
    "\n",
    "optimizer = tt.optim.Adam(model3.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "_uuid": "27d6f99a3600ca93afd9b7077406b9c809314ffd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb17537d091448b39ddf9148fbad783a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.827635430007668\n",
      "5.33970571906651\n",
      "5.206314249646281\n",
      "5.198968787425641\n",
      "5.239486333616171\n",
      "5.1483137585106755\n",
      "4.963710547342833\n",
      "5.19802997387168\n",
      "5.145028971493174\n",
      "5.0719664779966545\n",
      "5.0055777476453445\n"
     ]
    }
   ],
   "source": [
    "nn_train3(model3, train, valid, criterion, optimizer, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "_uuid": "5a85b1d06e4adda32403455a7a19886dae8dc349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spratorve\n",
      "toryst\n",
      "dormende\n",
      "sophol\n",
      "sthatoreri\n",
      "corek\n",
      "oryent\n",
      "kaalhot\n",
      "nesparum\n",
      "viganli\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampling(model3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "737c0e32aad9aafed76aaf67e897f81c12b1f6d2"
   },
   "outputs": [],
   "source": [
    "with open('model_drop_1000.pickle', 'wb') as handle:\n",
    "    pickle.dump(model3, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15db9fe4ab4ea0180025e5b1d6c222fcfd7bb964"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23cf3a24b613a03cdc879d593cd403ac9100d62f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d920e9409a7b308d07b0a0b96ff52120b2c5c67d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19cd2b90f622ba9e0d05f47a3e2756414ce64319"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
