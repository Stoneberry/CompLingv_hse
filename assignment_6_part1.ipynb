{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop RNN model in pytorch to solve the following problem:\n",
    "\n",
    "Detect sarcasm Data from https://www.kaggle.com/sherinclaudia/sarcastic-comments-on-reddit Your quality metric = accuracy Randomly select 20% of your data for test set. You can use it only for final perfomance estimation. Remember, you can use GPU resourses in kaggle kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "1a8c85f228bd1385d9f5961fd81dfe23fb4cb0a1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import nltk\n",
    "import gensim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Field, LabelField, BucketIterator, ReversibleField, TabularDataset\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "e57c85a8c5e11576f674700185f29716ef4393a3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала посмотрим на данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/Stoneberry/Desktop/Uni/Прога/4 курс/Комп/train-balanced-sarcasm.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    505413\n",
       "0    505413\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    505405\n",
       "1    505368\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть теория, что parent_comment и subreddit тоже могут влиять на качество классификации, поэтому создадим дополнительную колонку, в которую соединим 3 столбца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "\n",
    "for i in df.values:\n",
    "    item = i[1].split()\n",
    "    item += i[9].split()\n",
    "    item += i[3].split()\n",
    "    item = ' '.join(item)\n",
    "    c.append(item)\n",
    "\n",
    "df['text'] = c\n",
    "df.to_csv('train-balanced-sarcasm_demo3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "40bde78e36437313d297bdeaab9c8f3fa3f79288"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8be188625097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpunct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'«»—…“”*№–'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_en.remove_pipe('tagger')\n",
    "spacy_en.remove_pipe('ner')\n",
    "\n",
    "\n",
    "def tokenizer1(text):\n",
    "    return [tok.lemma_ for tok in spacy_en.tokenizer(text)]\n",
    "    #words = [word for word in text.lower().split()]\n",
    "    #return words\n",
    "\n",
    "def tokenizer2(text):\n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "5f2abea953f536551e2f58955505926662604d38"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, w2v=False, drop=False, mod='text'):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        if w2v is True:\n",
    "            global weights \n",
    "            self.embedding = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "           # tt.nn.init.uniform_(self.embedding.weight)\n",
    "        \n",
    "        self.mod = mod\n",
    "        \n",
    "        if drop is True:\n",
    "            self.drop_en = nn.Dropout(p=0.6)\n",
    "        else:\n",
    "            self.drop_en = False\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                          )\n",
    "        self.fc = nn.Linear(hidden_size * 2 *2, 2)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        if self.mod == 'text':\n",
    "            x, x_lengths = batch.text\n",
    "        if self.mod == 'comment':\n",
    "            x, x_lengths = batch.comment\n",
    "        if self.mod == 'parent_comment':\n",
    "            x, x_lengths = batch.parent_comment\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        if self.drop_en is not False:\n",
    "            x = self.drop_en(x)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.view(-1).tolist()\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n",
    "            \n",
    "        _, (hidden, cell) = self.rnn(x)\n",
    "        \n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden.contiguous().view(hidden.size(0),-1)\n",
    "        cell = cell.contiguous().view(cell.size(0),-1)\n",
    "        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "74b4cbe00110770fd59c8203c4dcbdcca2148f9c"
   },
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.label)\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0, cri=False):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, criterion, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator, criterion)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)\n",
    "        \n",
    "        if cri is True:\n",
    "            scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0fa5b73de8e6352ee59f9ce83fe7faebf0571e63"
   },
   "outputs": [],
   "source": [
    "def predict(batch, model, proba=True):\n",
    "    \n",
    "    global TEXT\n",
    "    prediction = tt.softmax(model.forward(batch), dim=-1)\n",
    "    prediction = prediction.detach().numpy()\n",
    "    \n",
    "    if proba is True: return prediction\n",
    "    else: return prediction.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2384d07ccaa9d897e91c8e3388e8d01fd27de86d"
   },
   "outputs": [],
   "source": [
    "def text_formaion(path, tok='tokenizer1', field='comment', max_size=30000):\n",
    "    \n",
    "    if tok == 'tokenizer1':\n",
    "        TEXT = Field(include_lengths=True, batch_first=True, \n",
    "                 tokenize=tokenizer1,\n",
    "                 eos_token='<eos>',\n",
    "                 lower=True,\n",
    "                 stop_words=nltk.corpus.stopwords.words('english')\n",
    "                )\n",
    "    else:\n",
    "        TEXT = Field(include_lengths=True, batch_first=True, \n",
    "                 tokenize=tokenizer2,\n",
    "                 eos_token='<eos>',\n",
    "                 lower=True,\n",
    "                 stop_words=nltk.corpus.stopwords.words('english')\n",
    "                )\n",
    "\n",
    "    LABEL = LabelField(dtype=tt.int64, use_vocab=False)\n",
    "\n",
    "    dataset = TabularDataset(path, format='csv', \n",
    "                         fields=[('label', LABEL), (field, TEXT)], \n",
    "                         skip_header=True)\n",
    "    \n",
    "    TEXT.build_vocab(dataset, min_freq=5, max_size=max_size)\n",
    "    print(len(TEXT.vocab.itos))\n",
    "    LABEL.build_vocab(dataset)\n",
    "    train, test = dataset.split(0.8, stratified=True)\n",
    "    train, valid = train.split(0.7, stratified=True)\n",
    "    \n",
    "    return TEXT, train, test, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "b9942b184de072f9a2691bd09bd11924594c4672"
   },
   "outputs": [],
   "source": [
    "def model_train(TEXT, train, valid, test, batch_size=100, w2v=False, drop=False, mod='text', n_epochs=10, embed_size=100, cri=False):\n",
    "    \n",
    "    batch_size = batch_size \n",
    "\n",
    "    model = MyModel(len(TEXT.vocab.itos),\n",
    "                embed_size=embed_size,\n",
    "                hidden_size=128,\n",
    "                w2v=w2v, drop=drop, mod=mod)\n",
    "\n",
    "    if mod == 'text':\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, valid, test),\n",
    "        batch_sizes=(batch_size, batch_size, batch_size), shuffle=True, sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,)\n",
    "    if mod == 'comment':\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, valid, test),\n",
    "        batch_sizes=(batch_size, batch_size, batch_size), shuffle=True, sort_key=lambda x: len(x.comment),\n",
    "        sort_within_batch=True,)\n",
    "    if mod == 'parent_comment':\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, valid, test),\n",
    "        batch_sizes=(batch_size, batch_size, batch_size), shuffle=True, sort_key=lambda x: len(x.parent_comment),\n",
    "        sort_within_batch=True,)\n",
    "        \n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler, \n",
    "        n_epochs=n_epochs, early_stopping=2, cri=cri)\n",
    "    \n",
    "    return model, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b5cdee765718e553aa12db6c2f1a891568177625"
   },
   "outputs": [],
   "source": [
    "def acc_score(model, test_iterator, proba=False):\n",
    "    res = []\n",
    "    t_par = tqdm_notebook(test_iterator, desc='i', leave=True)\n",
    "\n",
    "    for i in t_par:\n",
    "        pred = predict(i, model, proba=proba)\n",
    "        res.append(accuracy_score(np.array(i.label), pred))\n",
    "\n",
    "    return np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "cfb08295b871845c97c5eab4387ea41cef219f43"
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "weights = tt.FloatTensor(w2v_model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "29ca3f94d0efbe4710fa1117ad3d5ec1c2d2a53d"
   },
   "outputs": [],
   "source": [
    "path = '../input/my-sarcasm/train-balanced-sarcasm2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "776399da4050d0eb870b98b15801e9c25efeec5b"
   },
   "source": [
    "Колонка - comment, с удалением пунктуации, первоначальный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "57402b9c7b446043c2ede8add2455c3d434605a7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2540480b356842d097e0567be3f3d47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.58251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546b5d3e0c364400a2b21a8288d2338b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.57621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d08812c76947aa8609da1f854f8ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.58867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3779b82afd6440c0814c461e65ff3361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.62901\n",
      "Early stopping! best epoch: 1 val 0.57621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8151487d7d614e2f809d8c706ad8bceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='i', max=2022, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6842496178401223"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT2, train2, test2, valid2 = text_formaion(path, tok='tokenizer2', field='comment', max_size=30000)\n",
    "model2, test_iterator2 = model_train(TEXT2, train2, valid2, test2, batch_size=100, w2v=False, drop=False, mod='comment', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model2, test_iterator2, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если посмотреть, на тексты, можно увидеть, что там знаки препинания могут тоже нести какую-то информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'For those wondering, those are not my pictures :P. However, I did want to pick epic landscapes given the artist name and the epicness of this mix :D')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label[233], df.comment[233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '***ACKCHUALLY*** THE TERM IS LEVICORPUS WHEN TALKING ABOUT BODIES')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label[434244], df.comment[434244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'Language, Cap... such a hypocrite')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label[305], df.comment[305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language, Cap... such a hypocrite'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.comment[305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label[305]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, улучшится ли результат, если не удалять пунктуацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3252b72f6f1dbb4194a7f076fb90eeb0e76d8490"
   },
   "source": [
    "Колонка - comment, без удаления пунктуации, первоначальный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "29c8cb1058add4cf9b5d49f97b908b317c5d45c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71765b4b1d846c89fcfcc7eb04c8217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.56358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbecf3e21f0349cfb9000f31f5a7c906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.55803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac5ac82653c45c782dbd20113a4d2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.56765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc42afb46dea4adabef82d87fc35a59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss 0.59915\n",
      "Early stopping! best epoch: 1 val 0.55803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3034e7d754dd4afe8a0991f96b9004d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='i', max=2022, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7018712346012048"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT, train, test, valid = text_formaion(path, tok='tokenizer1', field='comment', max_size=30000)\n",
    "model, test_iterator = model_train(TEXT, train, valid, test, batch_size=100, w2v=False, drop=False, mod='comment', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model, test_iterator, proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5565a8a04bb17801b81a2c78287043f590288c0a"
   },
   "source": [
    "#### Удалось побить бейзлайн! Попробуем еще несколько вариантов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как в Kaggle сессия длится ограниченное время, ноутбуки постоянно вылетали, поэтому разные вещи я считала в нескольких разных кернелах. Если успеет досчитаться, выложу отдельными файлами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d135acbafe71112dd53929e0fb43636f1b1e4b40"
   },
   "source": [
    "Колонка - comment, без удаления пунктуации, первоначальный код, w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcc3906e55cdf351fd9d98d891fb91d38581f384"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8da0d43ab5040158deba08938352e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, test_iterator = model_train(TEXT, train, valid, test, batch_size=100, w2v=True, drop=False, mod='comment', n_epochs=10, embed_size=300, cri=False)\n",
    "acc_score(model, test_iterator, proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eec10bade61e817d687931591feb9321b4354813"
   },
   "source": [
    "Колонка - comment, без удаления пунктуации, измененный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "001ec37123b778ee806e040a6e1393bfcef6bcec"
   },
   "outputs": [],
   "source": [
    "model, test_iterator = model(TEXT, train, valid, test, batch_size=100, w2v=False, drop=True, mod='comment', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model, test_iterator, proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8cca7f291d4ceb83412923dad9eac8e17852fec"
   },
   "source": [
    "Колонка - comment, без удаления пунктуации, измененный код, в2в"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20b7ef5df8fd3f27422b415753296c7d235d31da"
   },
   "outputs": [],
   "source": [
    "model, test_iterator = model(TEXT, train, valid, test, batch_size=100, w2v=True, drop=True, mod='comment', n_epochs=10, embed_size=300, cri=False)\n",
    "acc_score(model, test_iterator, proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96352d202d0212c1342c312aa92523ed92a62b10"
   },
   "source": [
    "Колонка - parent+subr+comment, без удаления пунктуации, первоначальный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01d5dbd0809092787343f696f53335b1644d7d95"
   },
   "outputs": [],
   "source": [
    "model, test_iterator = model(TEXT, train, valid, test, batch_size=100, w2v=False, drop=False, mod='text', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model, test_iterator, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возможно, также влияет регистр текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formaion2(path, tok='tokenizer1', field='comment', max_size=30000):\n",
    "    \n",
    "    if tok == 'tokenizer1':\n",
    "        TEXT = Field(include_lengths=True, batch_first=True, \n",
    "                 tokenize=tokenizer1,\n",
    "                 eos_token='<eos>',\n",
    "                 lower=False,\n",
    "                 stop_words=nltk.corpus.stopwords.words('english')\n",
    "                )\n",
    "    else:\n",
    "        TEXT = Field(include_lengths=True, batch_first=True, \n",
    "                 tokenize=tokenizer2,\n",
    "                 eos_token='<eos>',\n",
    "                 lower=False,\n",
    "                 stop_words=nltk.corpus.stopwords.words('english')\n",
    "                )\n",
    "\n",
    "    LABEL = LabelField(dtype=tt.int64, use_vocab=False)\n",
    "\n",
    "    dataset = TabularDataset(path, format='csv', \n",
    "                         fields=[('label', LABEL), (field, TEXT)], \n",
    "                         skip_header=True)\n",
    "    \n",
    "    TEXT.build_vocab(dataset, min_freq=5, max_size=max_size)\n",
    "    print(len(TEXT.vocab.itos))\n",
    "    LABEL.build_vocab(dataset)\n",
    "    train, test = dataset.split(0.8, stratified=True)\n",
    "    train, valid = train.split(0.7, stratified=True)\n",
    "    \n",
    "    return TEXT, train, test, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Колонка - comment, с удалением пунктуации, сохранение капса, первоначальный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_formaion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-82e57c216393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEXT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_formaion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenizer2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0macc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_formaion' is not defined"
     ]
    }
   ],
   "source": [
    "TEXT2, train2, test2, valid2 = text_formaion2(path, tok='tokenizer2', field='comment', max_size=30000)\n",
    "model2, test_iterator2 = model_train(TEXT2, train2, valid2, test2, batch_size=100, w2v=False, drop=False, mod='comment', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model2, test_iterator2, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Колонка - comment, без удаления пунктуации, сохранение капса, первоначальный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT, train, test, valid = text_formaion2(path, tok='tokenizer1', field='comment', max_size=30000)\n",
    "model, test_iterator = model_train(TEXT, train, valid, test, batch_size=100, w2v=False, drop=False, mod='comment', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model, test_iterator, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
