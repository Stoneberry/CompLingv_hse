{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "1a8c85f228bd1385d9f5961fd81dfe23fb4cb0a1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import nltk\n",
    "import gensim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Field, LabelField, BucketIterator, ReversibleField, TabularDataset\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "e57c85a8c5e11576f674700185f29716ef4393a3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "40bde78e36437313d297bdeaab9c8f3fa3f79288"
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_en.remove_pipe('tagger')\n",
    "spacy_en.remove_pipe('ner')\n",
    "\n",
    "\n",
    "def tokenizer1(text):\n",
    "    return [tok.lemma_ for tok in spacy_en.tokenizer(text)]\n",
    "    #words = [word for word in text.lower().split()]\n",
    "    #return words\n",
    "\n",
    "def tokenizer2(text):\n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "5f2abea953f536551e2f58955505926662604d38"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, w2v=False, drop=False, mod='text'):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        if w2v is True:\n",
    "            global weights \n",
    "            self.embedding = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "           # tt.nn.init.uniform_(self.embedding.weight)\n",
    "        \n",
    "        self.mod = mod\n",
    "        \n",
    "        if drop is True:\n",
    "            self.drop_en = nn.Dropout(p=0.6)\n",
    "        else:\n",
    "            self.drop_en = False\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                          )\n",
    "        self.fc = nn.Linear(hidden_size * 2 *2, 2)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        if self.mod == 'text':\n",
    "            x, x_lengths = batch.text\n",
    "        if self.mod == 'comment':\n",
    "            x, x_lengths = batch.comment\n",
    "        if self.mod == 'parent_comment':\n",
    "            x, x_lengths = batch.parent_comment\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        if self.drop_en is not False:\n",
    "            x = self.drop_en(x)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.view(-1).tolist()\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n",
    "            \n",
    "        _, (hidden, cell) = self.rnn(x)\n",
    "        \n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden.contiguous().view(hidden.size(0),-1)\n",
    "        cell = cell.contiguous().view(cell.size(0),-1)\n",
    "        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "74b4cbe00110770fd59c8203c4dcbdcca2148f9c"
   },
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.label)\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0, cri=False):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, criterion, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator, criterion)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)\n",
    "        \n",
    "        if cri is True:\n",
    "            scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0fa5b73de8e6352ee59f9ce83fe7faebf0571e63"
   },
   "outputs": [],
   "source": [
    "def predict(batch, model, proba=True):\n",
    "    \n",
    "    global TEXT\n",
    "    prediction = tt.softmax(model.forward(batch), dim=-1)\n",
    "    prediction = prediction.detach().numpy()\n",
    "    \n",
    "    if proba is True: return prediction\n",
    "    else: return prediction.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2384d07ccaa9d897e91c8e3388e8d01fd27de86d"
   },
   "outputs": [],
   "source": [
    "def text_formaion(path, tok='tokenizer1', field='comment', max_size=30000):\n",
    "    \n",
    "    if tok == 'tokenizer1':\n",
    "        TEXT = Field(include_lengths=True, batch_first=True, \n",
    "                 tokenize=tokenizer1,\n",
    "                 eos_token='<eos>',\n",
    "                 lower=False,\n",
    "                 stop_words=nltk.corpus.stopwords.words('english')\n",
    "                )\n",
    "    else:\n",
    "        TEXT = Field(include_lengths=True, batch_first=True, \n",
    "                 tokenize=tokenizer2,\n",
    "                 eos_token='<eos>',\n",
    "                 lower=False,\n",
    "                 stop_words=nltk.corpus.stopwords.words('english')\n",
    "                )\n",
    "\n",
    "    LABEL = LabelField(dtype=tt.int64, use_vocab=False)\n",
    "\n",
    "    dataset = TabularDataset(path, format='csv', \n",
    "                         fields=[('label', LABEL), (field, TEXT)], \n",
    "                         skip_header=True)\n",
    "    \n",
    "    TEXT.build_vocab(dataset, min_freq=5, max_size=max_size)\n",
    "    print(len(TEXT.vocab.itos))\n",
    "    LABEL.build_vocab(dataset)\n",
    "    train, test = dataset.split(0.8, stratified=True)\n",
    "    train, valid = train.split(0.7, stratified=True)\n",
    "    \n",
    "    return TEXT, train, test, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b9942b184de072f9a2691bd09bd11924594c4672"
   },
   "outputs": [],
   "source": [
    "def model_train(TEXT, train, valid, test, batch_size=100, w2v=False, drop=False, mod='text', n_epochs=10, embed_size=100, cri=False):\n",
    "    \n",
    "    batch_size = batch_size \n",
    "\n",
    "    model = MyModel(len(TEXT.vocab.itos),\n",
    "                embed_size=embed_size,\n",
    "                hidden_size=128,\n",
    "                w2v=w2v, drop=drop, mod=mod)\n",
    "\n",
    "    if mod == 'text':\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, valid, test),\n",
    "        batch_sizes=(batch_size, batch_size, batch_size), shuffle=True, sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,)\n",
    "    if mod == 'comment':\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, valid, test),\n",
    "        batch_sizes=(batch_size, batch_size, batch_size), shuffle=True, sort_key=lambda x: len(x.comment),\n",
    "        sort_within_batch=True,)\n",
    "    if mod == 'parent_comment':\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, valid, test),\n",
    "        batch_sizes=(batch_size, batch_size, batch_size), shuffle=True, sort_key=lambda x: len(x.parent_comment),\n",
    "        sort_within_batch=True,)\n",
    "        \n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler, \n",
    "        n_epochs=n_epochs, early_stopping=2, cri=cri)\n",
    "    \n",
    "    return model, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b5cdee765718e553aa12db6c2f1a891568177625"
   },
   "outputs": [],
   "source": [
    "def acc_score(model, test_iterator, proba=False):\n",
    "    res = []\n",
    "    t_par = tqdm_notebook(test_iterator, desc='i', leave=True)\n",
    "\n",
    "    for i in t_par:\n",
    "        pred = predict(i, model, proba=proba)\n",
    "        res.append(accuracy_score(np.array(i.label), pred))\n",
    "\n",
    "    return np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "cfb08295b871845c97c5eab4387ea41cef219f43"
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "weights = tt.FloatTensor(w2v_model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "29ca3f94d0efbe4710fa1117ad3d5ec1c2d2a53d"
   },
   "outputs": [],
   "source": [
    "path = '../input/my-sarcasm/train-balanced-sarcasm2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b80769bb8099dc784c60a94ee3561e69eef2b1b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "141618604d18e9f3eda39fe194ee360737b4ed2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53af13fea76712f38f9d512582e10576eb34965c"
   },
   "source": [
    "Колонка - comment, с удалением пунктуации, сохранение капса, первоначальный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc1a089fa8e67df8ef7ee6dcf137fad176daa792",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT2, train2, test2, valid2 = text_formaion(path, tok='tokenizer2', field='comment', max_size=30000)\n",
    "model2, test_iterator2 = model_train(TEXT2, train2, valid2, test2, batch_size=100, w2v=False, drop=False, mod='comment', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model2, test_iterator2, proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3252b72f6f1dbb4194a7f076fb90eeb0e76d8490"
   },
   "source": [
    "Колонка - comment, без удаления пунктуации, сохранение капса, первоначальный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "29c8cb1058add4cf9b5d49f97b908b317c5d45c5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295a034c57c54cbf8b90a86d018b14c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.55293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2f5f297e04c5a9bd94c1a38ef28c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.54385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a82b7013dfa4e0dae679f83ee11089a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.55331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d758dba0e144feb3065cf7897687fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=5661, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.58325\n",
      "Early stopping! best epoch: 1 val 0.54385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20daa0d9e634907add16ed74b1eac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='i', max=2022, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7127056919341785"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT, train, test, valid = text_formaion(path, tok='tokenizer1', field='comment', max_size=30000)\n",
    "model, test_iterator = model_train(TEXT, train, valid, test, batch_size=100, w2v=False, drop=False, mod='comment', n_epochs=10, embed_size=100, cri=False)\n",
    "acc_score(model, test_iterator, proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb3a10055108e4d5a6d6316e94fe5f0d194485de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "714eaf57229eaa4d9f0675627d973c2109bcd721"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f226f01f7f2140b92ea5a4a2daebfa7538c09f2f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05ecd71c61e6633122a937836430feafe877ea6d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c22fa3e9150f38cf5ec2d4e1671ac9937b4c5303"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fbe2fe727df19fb0a7f559fad6ff9adcf5a8e08"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52840d6ce122450ccb9eab776e86f3297635a2a5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db40cbd92e4ffc59d1d135ded03f002580a96cc8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53614709a6b247e447b18d34bf11e138ebd67bca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eaa8d75a10a1d254ffe70ea812e938480834a44e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8dd8afe99b1e5fc7c1099b8e2204f15140955dd8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d093ea5102fda2c7b1ffb305774a7e361596e47f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40c430d158c3fcda2d05138a337ea28ec79fe849"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1415e84f2c14130e7774724307def4cbdcd9f6e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b3a5047d90594ab683e78832334e17b5db68408"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fc1fe060e1efedc49573f37717bf6161676c673"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01bdb449988f106421cf6181476435b034ad4d08"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
